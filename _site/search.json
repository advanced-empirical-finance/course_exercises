[
  {
    "objectID": "machine_learning.html",
    "href": "machine_learning.html",
    "title": "Machine Learning 1: Shrinkage Estimation",
    "section": "",
    "text": "In this Chapter, you get to know tidymodels (R) and scikit-learn (Python), a collection of packages for modeling and machine learning (ML) using tidy coding principles. From a finance perspective, you will apply penalized regressions to understand which factors and macroeconomic predictors may help to explain the cross-section of industry returns.\nExercises: \n\nIn this analysis, we use four different data sources. Load the monthly Fama-French 3-factor returns, the monthly q-factor returns from Hou, Xue, and Zhang (2014), the macroeconomic predictors from Welch and Goyal (2008), and monthly portfolio returns from 10 different industries according to the definition from Kenneth French’s homepage as test assets. Your data should contain 22 columns of regressors with 13 macro variables and 8-factor returns for each month.\n\nProvide meaningful summary statistics for the test assets’ excess returns.\n\n\nRPython\n\n\n\nFamiliarize yourself with the tidymodels workflow. First, restrict yourself to just one industry, e.g. Manufacturing. Use the function initial_time_split from the rsample package to split the sample into a training and a test set.\nPreprocess the data by creating a recipe which performs the following steps: First, the aim is to explain the industry excess returns as a function of all predictors. Exclude the column month from the analysis. Include all interaction terms between factors and macroeconomic predictors. Demean and scale each regressor such that the standard deviation is one.\nBuild a model linear_reg with tidymodels with a fixed penalty term that performs lasso regression. Create a workflow that first applies the recipe steps to the training data and then fits the model. Illustrate the predicted industry returns for the in-sample and the out-of-sample period.\nNext, tune the model such that the penalty term is flexibly chosen by cross-validation. For that purpose, update the model such that both, penalty and mixture are flexible tuning variables. Use the function time_series_cv to generate a time series cross-validation sample which allows tuning the model with 20 random samples of length five years with a validation period of four years. Tune the model for a grid of possible penalty and mixture values and visualize the mean-squared prediction errors for the industry returns across the range of possible tuning parameters.\nFinally, write a function to parallelize the entire workflow. The function should split the data into a training and test set, create a cross-validation scheme, and tunes a lasso model for different penalty values. Finally, select the best model in terms of MSPE in the validation test set. Apply the function to every individual industry and illustrate for each industry which factor and macroeconomic variables are selected by the Lasso.\n\n\n\n\nFamiliarize yourself with the scikit-learn workflow. First, restrict yourself to just one industry, e.g. Manufacturing. Use the function train_test_split to split the sample into a training and a test set.\nPreprocess the data with ColumnTransformer which performs the following steps: First, the aim is to explain the industry excess returns as a function of all predictors. Exclude the column month from the analysis. Include all interaction terms between factors and macroeconomic predictors. Demean and scale each regressor such that the standard deviation is one.\nBuild a model ElasticNet with sklearn.linear_model with a fixed penalty term that performs lasso regression. Create a Pipeline that first applies the recipe steps to the training data and then fits the model. Illustrate the predicted industry returns for the in-sample and the out-of-sample period.\nNext, tune the model such that the penalty term is flexibly chosen by cross-validation. For that purpose, update the model such that both, penalty and mixture are flexible tuning variables. Use the function TimeSeriesSplit to generate a time series cross-validation sample which allows tuning the model with 20 random samples of length five years with a validation period of four years. Tune the model for a grid of possible penalty and mixture values and visualize the mean-squared prediction errors for the industry returns across the range of possible tuning parameters.\nFinally, write a function to parallelize the entire workflow. The function should split the data into a training and test set, create a cross-validation scheme, and tunes a lasso model for different penalty values. Finally, select the best model in terms of MSPE in the validation test set. Apply the function to every individual industry and illustrate for each industry which factor and macroeconomic variables are selected by the Lasso.\n\n\n\n\nSolutions:  All solutions are provided in the book chapter Factor selection via machine learning (R or Python version).",
    "crumbs": [
      "Machine Learning",
      "Machine Learning 1: Shrinkage Estimation"
    ]
  },
  {
    "objectID": "machine_learning.html#factor-selection-via-machine-learning",
    "href": "machine_learning.html#factor-selection-via-machine-learning",
    "title": "Machine Learning 1: Shrinkage Estimation",
    "section": "",
    "text": "In this Chapter, you get to know tidymodels (R) and scikit-learn (Python), a collection of packages for modeling and machine learning (ML) using tidy coding principles. From a finance perspective, you will apply penalized regressions to understand which factors and macroeconomic predictors may help to explain the cross-section of industry returns.\nExercises: \n\nIn this analysis, we use four different data sources. Load the monthly Fama-French 3-factor returns, the monthly q-factor returns from Hou, Xue, and Zhang (2014), the macroeconomic predictors from Welch and Goyal (2008), and monthly portfolio returns from 10 different industries according to the definition from Kenneth French’s homepage as test assets. Your data should contain 22 columns of regressors with 13 macro variables and 8-factor returns for each month.\n\nProvide meaningful summary statistics for the test assets’ excess returns.\n\n\nRPython\n\n\n\nFamiliarize yourself with the tidymodels workflow. First, restrict yourself to just one industry, e.g. Manufacturing. Use the function initial_time_split from the rsample package to split the sample into a training and a test set.\nPreprocess the data by creating a recipe which performs the following steps: First, the aim is to explain the industry excess returns as a function of all predictors. Exclude the column month from the analysis. Include all interaction terms between factors and macroeconomic predictors. Demean and scale each regressor such that the standard deviation is one.\nBuild a model linear_reg with tidymodels with a fixed penalty term that performs lasso regression. Create a workflow that first applies the recipe steps to the training data and then fits the model. Illustrate the predicted industry returns for the in-sample and the out-of-sample period.\nNext, tune the model such that the penalty term is flexibly chosen by cross-validation. For that purpose, update the model such that both, penalty and mixture are flexible tuning variables. Use the function time_series_cv to generate a time series cross-validation sample which allows tuning the model with 20 random samples of length five years with a validation period of four years. Tune the model for a grid of possible penalty and mixture values and visualize the mean-squared prediction errors for the industry returns across the range of possible tuning parameters.\nFinally, write a function to parallelize the entire workflow. The function should split the data into a training and test set, create a cross-validation scheme, and tunes a lasso model for different penalty values. Finally, select the best model in terms of MSPE in the validation test set. Apply the function to every individual industry and illustrate for each industry which factor and macroeconomic variables are selected by the Lasso.\n\n\n\n\nFamiliarize yourself with the scikit-learn workflow. First, restrict yourself to just one industry, e.g. Manufacturing. Use the function train_test_split to split the sample into a training and a test set.\nPreprocess the data with ColumnTransformer which performs the following steps: First, the aim is to explain the industry excess returns as a function of all predictors. Exclude the column month from the analysis. Include all interaction terms between factors and macroeconomic predictors. Demean and scale each regressor such that the standard deviation is one.\nBuild a model ElasticNet with sklearn.linear_model with a fixed penalty term that performs lasso regression. Create a Pipeline that first applies the recipe steps to the training data and then fits the model. Illustrate the predicted industry returns for the in-sample and the out-of-sample period.\nNext, tune the model such that the penalty term is flexibly chosen by cross-validation. For that purpose, update the model such that both, penalty and mixture are flexible tuning variables. Use the function TimeSeriesSplit to generate a time series cross-validation sample which allows tuning the model with 20 random samples of length five years with a validation period of four years. Tune the model for a grid of possible penalty and mixture values and visualize the mean-squared prediction errors for the industry returns across the range of possible tuning parameters.\nFinally, write a function to parallelize the entire workflow. The function should split the data into a training and test set, create a cross-validation scheme, and tunes a lasso model for different penalty values. Finally, select the best model in terms of MSPE in the validation test set. Apply the function to every individual industry and illustrate for each industry which factor and macroeconomic variables are selected by the Lasso.\n\n\n\n\nSolutions:  All solutions are provided in the book chapter Factor selection via machine learning (R or Python version).",
    "crumbs": [
      "Machine Learning",
      "Machine Learning 1: Shrinkage Estimation"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!",
    "section": "",
    "text": "In the course “Advanced Empirical Finance” we repeatedly ask: (How) can state-of-the-art methods improve financial decision-making?\nWhile the lecture covers all relevant theoretical aspects and is based on very recent academic papers, you should spend most of your effort on this course on actually doing empirical work! Get your computer ready to work on real problems for financial applications and discuss your code with your peers to acquire the necessary skills to make a difference either in the Finance industry or academia.\nThis set of exercises is written for the students of the lecture “Advanced Empirical Finance: Topics and Data Science”. The exercise sets partially subsume work I created with my colleagues Christoph Scheuch and Patrick Weiss for the books Tidy Finance with R and Tidy Finance with Python. You are very welcome to give us feedback on every aspect of the book such that we can improve the codes, explanations, and general structure. Please contact me or my colleagues directly via contact@tidy-finance.org if you spot typos, mistakes, or other issues that deserve more attention.\nNeedless to say, you should try to solve each question on your own before you refer to my proposed answers. Optimally, you discuss issues with your peers and try to find hints either in the lecture slides or online. There are many ways to get an answer to your questions, most directly you can simply post your questions on StackExchange or Absalon.\nAs an absolute minimum before trying to solve the following exercises, make sure you are familiar with Garrett Grolemund’s and Hadley Wickham’s excellent book R for Data Science. I will make further information or references available on Absalon.",
    "crumbs": [
      "Introduction",
      "Welcome!"
    ]
  },
  {
    "objectID": "index.html#things-to-get-done-before-the-first-lecture",
    "href": "index.html#things-to-get-done-before-the-first-lecture",
    "title": "Welcome!",
    "section": "Things to get done before the first lecture",
    "text": "Things to get done before the first lecture\nTo dive right into it, check the readme file to comply with the technical prerequisites of this course - you should have R, Quarto, and RStudio ready to get started.",
    "crumbs": [
      "Introduction",
      "Welcome!"
    ]
  },
  {
    "objectID": "asset_pricing.html",
    "href": "asset_pricing.html",
    "title": "Asset Pricing",
    "section": "",
    "text": "Beta Estimation\nExercises:\n\nRead in the clean CRSP data (crsp_monthly) set from the tidy_finance_*.sqlite file (if you do not recall how to do this, check the previous chapter)\nRead in the Fama-French monthly market returns (factors_ff_monthly) from the database\nCompute the market beta \\(\\beta_\\text{AAPL}\\) of ticker AAPL (permno == 14593). You can use the function lm() (R) or smf.ols (Python) for that purpose (alternatively: compute the well-known OLS estimate \\((X'X)^{-1}X'Y\\) on your own).\nFor monthly data, it is common to compute \\(\\beta_i\\) based on a rolling window of length five years. Implement a rolling procedure that estimates assets market beta each month based on the last 60 observations. You can use the package slider (R), statsmodels.regression.rolling (Python), or a simple for loop. (Note: this is going to be a time-consuming computational task)\nStore the beta estimates in the tidy_finance_*.sqlite database as beta_exercise (the file tidy_finance_*.sqlite already contains a table beta with the estimated values from the textbook - it may be a good idea to compare your results with the ones we get).\nProvide summary statistics for the cross-section of estimated betas\nWhat is the theoretical prediction of CAPM concerning the relationship between market beta and expected returns? What would you expect if you create portfolios based on beta (you create a high- and a low-beta portfolio each month and track the performance over time)? How should the expected returns differ between high and low-beta portfolios?\n\nSolutions: All solutions are provided in the book chapter Beta estimation (R version) or Beta estimation (Python version)\n\n\nUnivariate sorts\nExercises:\n\nLoad the monthly CRSP file, the Fama-French Factors, and the estimated betas from the tidy_finance_*.sqlite database.\nCreate portfolio sorts based on the lagged beta. Specifically, you compute the breakpoint as the median lag beta each month. Then, you compute the returns of a portfolio that invests only in the stocks with a higher beta than the breakpoint and a portfolio that invests only in the stocks with a lower beta than the breakpoints. The portfolio weights can either be equal or value-weighted.\nWhat are the monthly excess returns of both portfolios?\nDoes a portfolio that goes long high beta stocks and short low beta stocks yield an excess return significantly different from zero?\nWrite a general function for portfolio sorts based on a variable number of breakpoints. Then, compute portfolio returns based on lagged beta decile sorts.\nWhat is the CAPM alpha of the ten portfolio returns? Is this finding in line with your expectations based on the CAPM implications?\nDoes a high beta minus low beta portfolio yield abnormal excess returns?\n\nSolutions: All solutions are provided in the book chapter Univariate portfolio sorts (R Version) or Univariate portfolio sorts (Python Version)",
    "crumbs": [
      "Asset Pricing",
      "Asset Pricing"
    ]
  },
  {
    "objectID": "financial_data.html",
    "href": "financial_data.html",
    "title": "Accessing & managing financial data",
    "section": "",
    "text": "Accessing & managing financial data\nExercises:\n\nRead the book chapters Accessing & managing financial data and WRDS, CRSP, and Compustat entirely. If you prefer to read Python code, the chapters are available here and here. They may contain some advanced concepts but also a description of almost every important dataset relevant to research in empirical finance.\nConsult the material on Absalon on how to get the raw CRSP data as a KU student. Download the raw monthly CRSP data from the Bloomberg terminals in building 26 and follow the cleaning steps described in Downloading and Preparing CRSP (R and Python). To get more information on how to compute returns adjusted for delisting, follow the procedure described in Chapter 7.2 of the book Empirical Asset Pricing.\nDownload the files tidy_finance_r.sqlite and tidy_finance_python.sqlite from Absalon. Optimally you store it in the folder called data within your standard working directory for the course. Almost all exercises from now on will start with reading data out of this file, so make sure you familiarize yourself with this short minimal setup to load data into your R or Python session memory from a fresh session (you can consult it anytime again later during the course).\n\n\nFrom now on, all you need to do to access data that is stored in the database is to follow three steps: (i) Establish the connection to the SQLite database, (ii) call the table you want to extract, and (iii) collect the data. For your convenience, the following steps show all you need compactly.\n\n\nPythonR\n\n\n\nimport pandas as pd\nimport sqlite3\n\ntidy_finance = sqlite3.connect(\n  database=\"../data/tidy_finance_python.sqlite\"\n)\n\ncrsp_monthly = (pd.read_sql_query(\n  sql=(\"SELECT permno, month, industry, ret_excess \" \n       \"FROM crsp_monthly\"),\n  con=tidy_finance_python,\n  parse_dates={\"month\"})\n  .dropna()\n)\n\n\n\n\n    library(tidyverse)\n    library(RSQLite)\n    tidy_finance &lt;- dbConnect(SQLite(), \n                              \"../data/tidy_finance_r.sqlite\", \n                              extended_types = TRUE)\n    factors_ff_monthly &lt;- tbl(tidy_finance, \"factors_ff3_monthly\") \n    factors_ff_monthly &lt;- factors_ff_monthly |&gt; collect()\n\n\n\n\n\nAs always (but this is important): If you need help with the SQL database, post your question on Absalon. Your TA, your peers, and I will help you!\nReplicate the following two figures provided in the lecture slides: i) Create a time series of the number of stocks in the CRSP sample, which are listed on NASDAQ, NYSE, and AMEX. ii) Illustrate the time series of total market values (inflation-adjusted) based on industry classification siccd. The book Empirical Asset Pricing (Bali, Murrey, and Engle) provides a detailed walk-through if you need help.\n\nSolutions: All solutions are provided in the book chapter WRDS, CRSP, and Compustat (R version and Python version) and the lecture slides.",
    "crumbs": [
      "Asset Pricing",
      "Accessing & managing financial data"
    ]
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction to Tidy Finance",
    "section": "",
    "text": "The main aim of this chapter is to familiarize yourself with tidy coding principles. We start by downloading and visualizing stock data before we move to a simple portfolio choice problem. These examples introduce you to our approach of Tidy Finance.",
    "crumbs": [
      "Introduction",
      "Introduction to Tidy Finance"
    ]
  },
  {
    "objectID": "introduction.html#working-with-stock-market-data",
    "href": "introduction.html#working-with-stock-market-data",
    "title": "Introduction to Tidy Finance",
    "section": "Working with stock market data",
    "text": "Working with stock market data\nExercises:\n\nDownload daily prices for one stock market ticker of your choice (e.g. AAPL) from Yahoo!Finance. Plot the time series of adjusted closing prices. To download the data with R you can use the command tq_get from the tidyquant package. If you do not know how to use it, make sure you read the help file by calling ?tq_get. I especially recommended taking a look at the examples section in the documentation. For Python, you may want to inspect the documentation of the yfinance package.\nCompute daily net returns for the asset and visualize the distribution of daily returns in a histogram using ggplot2 (R) or plotnine (Python). Also, use geom_vline() to add a dashed red line that indicates the 5% quantile of the daily returns within the histogram.\nCompute summary statistics (mean, standard deviation, minimum and maximum) for the daily returns.\n\nSolutions: All tasks are solved and described in detail in Chapter 1.1 in Tidy Finance with R. For a Python version of the code, consult Chapter 1.1 in Tidy Finance with Python",
    "crumbs": [
      "Introduction",
      "Introduction to Tidy Finance"
    ]
  },
  {
    "objectID": "introduction.html#scaling-up-the-analysis",
    "href": "introduction.html#scaling-up-the-analysis",
    "title": "Introduction to Tidy Finance",
    "section": "Scaling up the analysis",
    "text": "Scaling up the analysis\nAs a next step, we generalize the code from before such that all the computations can handle an arbitrary vector of tickers (e.g., all constituents of an index). Following tidy principles, it is quite easy to download the data, plot the price time series, and tabulate the summary statistics for an arbitrary number of assets.\nExercises:\n\nNow the tidyverse magic starts: Take your code from before and generalize it such all the computations are performed for an arbitrary vector of tickers (e.g., ticker &lt;- c(\"AAPL\", \"MMM\", \"BA\") (R) or ticker = [\"AAPL\", \"MMM\", \"BA\"] (Python)). You can also call ticker &lt;- tq_index(\"DOW\") (R) to get daily data from all constituents of the Dow Jones index. Automate the download, the plot of the price time series, and create a table of return summary statistics for this arbitrary number of assets.\nConsider the research question Are days with high aggregate trading volume often followed by high aggregate trading volume days? How would you investigate this question? Hint: Compute the aggregate daily trading volume (in USD) and find an appropriate visualization to analyze the question.\n\nSolutions: All tasks are solved and described in detail in Tidy Finance with R in the Chapters Scaling up the analysis and Other forms of data aggregation and in Tidy with Python in the Chapters Scaling up the analysis and Other forms of data aggregation.",
    "crumbs": [
      "Introduction",
      "Introduction to Tidy Finance"
    ]
  },
  {
    "objectID": "introduction.html#portfolio-choice-problems",
    "href": "introduction.html#portfolio-choice-problems",
    "title": "Introduction to Tidy Finance",
    "section": "Portfolio choice problems",
    "text": "Portfolio choice problems\nExercises:\n\nCompute monthly returns from all adjusted daily Dow Jones constituent prices. Hint for Python users: You can download the constituents of the Dow Jones 30 index from this homepage.\nCompute the vector of historical average returns and the sample variance-covariance matrix.\nCompute the minimum variance portfolio weights and the portfolio volatility and average returns.\nVisualize the mean-variance efficient frontier. For that, first, compute the efficient portfolio for an arbitrary risk-free rate (below the average return of the minimum variance portfolio). Then use the two-fund separation theorem to compute the location of a grid of combinations of the minimum variance portfolio and the efficient portfolio on the mean-volatility diagram which has (annualized) volatility on the \\(x\\) axis and (annualized) expected returns on the \\(y\\) axis. Indicate the location of the individual assets on the mean-volatility diagram as well.\n\nSolutions: All tasks are solved and described in detail in Tidy Finance with R and Tidy Finance with Python, in the Chapters Portfolio Choice Problems and The Efficient Frontier.",
    "crumbs": [
      "Introduction",
      "Introduction to Tidy Finance"
    ]
  },
  {
    "objectID": "introduction.html#equivalence-between-certainty-equivalent-maximization-and-minimum-variance-optimization",
    "href": "introduction.html#equivalence-between-certainty-equivalent-maximization-and-minimum-variance-optimization",
    "title": "Introduction to Tidy Finance",
    "section": "Equivalence between Certainty equivalent maximization and minimum variance optimization",
    "text": "Equivalence between Certainty equivalent maximization and minimum variance optimization\nIn the lecture slides (Parameter uncertainty), I argue that an investor with a quadratic utility function with a certainty equivalent \\[\\max_w CE(w) = \\omega'\\mu - \\frac{\\gamma}{2} \\omega'\\Sigma \\omega \\text{ s.t. } \\iota'\\omega = 1\\] faces an equivalent optimization problem to a framework where portfolio weights are chosen to minimize volatility given a pre-specified level or expected returns \\[\\min_w \\omega'\\Sigma \\omega \\text{ s.t. } \\omega'\\mu = \\bar\\mu \\text{ and } \\iota'\\omega = 1\\] Note the differences: In the first case, the investor has a (known) risk aversion \\(\\gamma\\) which determines her optimal balance between risk (\\(\\omega'\\Sigma\\omega)\\) and return (\\(\\mu'\\omega\\)). In the second case, the investor has a target return she wants to achieve while minimizing the volatility. Intuitively, both approaches are closely connected if we consider that the risk aversion \\(\\gamma\\) determines the desirable return \\(\\bar\\mu\\). More risk-averse investors (higher \\(\\gamma\\)) will choose a lower target return to keep their volatility level down. The efficient frontier then spans all possible portfolios depending on the risk aversion \\(\\gamma\\), starting from the minimum variance portfolio (\\(\\gamma = \\infty\\)).\nExercises:  Proof that the optimal portfolio weights are equivalent in both cases.\nSolutions: The proofs are provided in the Appendix of Tidy Finance with R.",
    "crumbs": [
      "Introduction",
      "Introduction to Tidy Finance"
    ]
  }
]